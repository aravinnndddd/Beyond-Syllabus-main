---

country: "india"
university: "ktu"
branch: "ece"
version: "2024"
semester: 7
course_code: "peect745"
course_title: "deep-learning-techniques"
language: "english"
contributor: "@arya3077"

---

# PEECT745 - Deep Learning Techniques

## Course Objectives

1. Provide foundational knowledge of advanced neural network architectures like CNNs, RNNs, and generative models.  
2. Offer practical insights into training, optimization, and applications in transfer learning and sequence modeling.  

---

## Syllabus Modules

### Module 1
**Review of ANN & CNN Basics**  
- Perceptrons.  
- Convolutional Neural Networks: convolution operation, CNN architecture, kernels, padding, convolutional layers, pooling layers, fully connected layers.  
- Feature and weight visualization, t-SNE.  

---

### Module 2
**CNN Training, Optimization, and Transfer Learning**  
- Loss functions: Mean Squared Error, Cross-Entropy.  
- Activation functions: Sigmoid, ReLU, Softmax.  
- Training CNNs: initialization, back-propagation.  
- Optimization algorithms: SGD, Momentum, Adagrad, RMSProp, Adam.  
- Hyperparameter optimization: learning rate.  
- Regularization methods: L1, L2 regularization, dropout, data augmentation, early stopping, batch normalization.  
- Transfer learning: feature extraction, fine-tuning.  
- Case studies (for practical assignments/microprojects only): AlexNet, VGG, ResNet, GoogleNet.  

---

### Module 3
**Sequence Models**  
- Recurrent Neural Networks (RNN): cell structure, architecture, training, back-propagation through time, vanishing and exploding gradients.  
- Long Short-Term Memory (LSTM): architecture, training.  
- Gated Recurrent Units (GRU): architecture, training.  

---

### Module 4
**Generative Models and Transformers**  
- Introduction to generative models: parameter estimation, Maximum Likelihood Estimation.  
- Autoencoders, latent space, variational autoencoders (VAE).  
- Generative Adversarial Networks (GANs): adversarial training, discriminator, generator, upsampling.  
- Transformer models: architecture, word embedding, position encoding, attention, training transformer models.  
- Large language models: BERT, GPT.  
- *(Detailed mathematical treatment not required)*  

---

## Reference Books

- *Deep Learning* – Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT Press, 2016.  
- *Neural Networks and Deep Learning: A Textbook* – Charu C. Aggarwal, Springer, 2019.  
- *Generative Deep Learning* – David Foster, O’Reilly, 2022.  
- *Build a Large Language Model* – Sebastian Raschka, Manning, 2023.  
- *Deep Learning with Python (2nd Edition)* – François Chollet, Manning, 2021.  
- *Learning Deep Learning* – Magnus Ekman, Addison-Wesley, 2022.  
- *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* – Aurélien Géron, O’Reilly, 2019.  
- *Dive into Deep Learning* – Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola, Cambridge University Press, 2019.  
- *Neural Networks and Deep Learning* – Michael Nielsen, available at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/), 2019.  

---
