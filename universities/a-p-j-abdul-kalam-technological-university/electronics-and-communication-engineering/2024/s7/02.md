---

country: "india"
university: "ktu"
branch: "ece"
version: "2024"
semester: 7
course_code: "peect742"
course_title: "deep-learning"
language: "english"
contributor: "@arya3077"

---

# PEECT742 - Deep Learning

## Course Objectives

1. Understand the theoretical basics of neural networks and deep learning.  

---

## Syllabus Modules

### Module 1
**Review of ANN and CNN Basics**  
- Review of artificial neural networks (ANN): perceptrons.  
- Convolutional Neural Networks (CNN): convolution operation, CNN architecture, kernels, padding.  
- Convolutional layers, pooling layers, fully connected layers.  
- Feature and weight visualization, t-SNE.  

---

### Module 2
**CNN Training, Optimization & Regularization**  
- Loss functions: Mean Squared Error, Cross-Entropy.  
- Activation functions: Sigmoid, ReLU, Softmax.  
- Training CNNs: initialization, backpropagation.  
- Optimization algorithms: SGD, Momentum, Adagrad, RMSProp, Adam.  
- Hyperparameter optimization: learning rate.  
- Regularization methods: L1, L2 regularization, dropout, data augmentation, early stopping, batch normalization.  
- Transfer learning: feature extraction, fine-tuning.  
- **Case studies** (for practical assignments/microprojects): AlexNet, VGG, ResNet, GoogleNet.  

---

### Module 3
**Sequence Models & Recurrent Architectures**  
- Sequence models, Recurrent Neural Networks (RNN): cell structure and architecture, training RNNs, backpropagation through time.  
- Challenges: vanishing and exploding gradients.  
- Long Short-Term Memory (LSTM): architecture and training.  
- Gated Recurrent Units (GRU): architecture and training.  

---

### Module 4
**Generative Models & Transformer Architectures**  
- Introduction to generative models: parameter estimation, Maximum Likelihood Estimation.  
- Autoencoders, latent space, variational autoencoders (VAE).  
- Generative Adversarial Networks (GANs): adversarial training, discriminator, generator, upsampling.  
- Transformer models: architecture, word embeddings, position encoding, attention mechanisms, training transformers.  
- Large Language Models (LLMs): BERT, GPT.  
> *Note: Detailed mathematical treatment not required for this module.*  

---

## Reference Books

- *Deep Learning* – Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT Press, 2016.  
- *Neural Networks and Deep Learning: A Textbook* – Charu C. Aggarwal, Springer, 2019.  
- *Generative Deep Learning* – David Foster, O’Reilly, 2022.  
- *Build a Large Language Model* – Sebastian Raschka, Manning, 2023.  
- *Learning Deep Learning* – Magnus Ekman, Addison-Wesley, 2022.  
- *Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow* – Aurelien Geron, O’Reilly, 2/e, 2019.  
- *Dive into Deep Learning* – Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola, Cambridge University Press, 2019, [https://d2l.ai/](https://d2l.ai/).  
- *Neural Networks for Deep Learning* – Michael Nielsen, 2019, [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/).  

---
