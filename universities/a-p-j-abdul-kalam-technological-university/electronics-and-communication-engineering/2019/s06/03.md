---
country: "india"
university: "ktu"
branch: "electronics-and-communication-engineering"
version: "2019"
semester: 6
course_code: "ect306"
course_title: "information-theory-and-coding"
language: "english"
contributor: "@alwynrejicser"
---

# ECT306: Information Theory and Coding

## Course Objectives

- To introduce fundamental concepts of information theory and quantify information content using entropy and mutual information.
- To understand and apply source and channel coding theorems for efficient and error-resilient communication.
- To design and analyze different coding techniques including block codes, cyclic codes, convolutional codes, and LDPC codes.
- To explore practical implications of the Shannon limit and the trade-offs in communication system design.

## Course Outcomes

- Explain measures of information – entropy, conditional entropy, mutual information.
- Apply Shannon’s source coding theorem for data compression.
- Apply the concept of channel capacity to characterize limits of error-free transmission.
- Apply linear block codes for error detection and correction.
- Apply algebraic codes with reduced structural complexity for error correction.
- Understand encoding and decoding of convolutional and LDPC codes.

## Course Content

### Module 1: Entropy, Sources and Source Coding

- Entropy and its properties
- Joint and conditional entropy
- Mutual information and its properties
- Discrete memoryless sources
- Source code, average length, and bounds on average length
- Uniquely decodable and prefix-free codes
- Kraft Inequality (with proof)
- Huffman coding
- Shannon’s source coding theorem (achievability, converse)
- Operational meaning of entropy

### Module 2: Channels and Channel Coding

- Discrete memoryless channels and channel capacity
- Binary Symmetric Channel (BSC) and Binary Erasure Channel (BEC)
- Capacity of BSC and BEC
- Channel coding, rate of channel code
- Shannon’s channel coding theorem (achievability, converse without proof)
- Operational meaning of channel capacity
- Additive White Gaussian Noise (AWGN) channels
- Continuous-input channels and average power constraint
- Differential entropy, entropy of Gaussian RV
- Shannon-Hartley theorem (with proof)
- Spectral efficiency vs SNR per bit
- Power-limited and bandwidth-limited regions
- Shannon limit and ultimate Shannon limit

### Module 3: Introduction to Linear Block Codes

- Basics of groups, rings, finite fields
- Construction of finite fields using polynomial rings
- Vector spaces
- Block codes: parameters, error detection and correction
- Linear block codes: repetition code, single parity-check code
- Generator and parity-check matrices
- Systematic form
- Maximum likelihood decoding
- Bounded distance decoding
- Syndrome decoding
- Standard array decoding

### Module 4: A Few Important Classes of Algebraic Codes

- Cyclic codes: polynomial and matrix representation
- Interrelation between representations
- Systematic encoding
- Decoding (descriptive only, no algorithms)
- Hamming Codes
- BCH Codes
- Reed-Solomon Codes

### Module 5: Convolutional and LDPC Codes

- Convolutional Codes: state and trellis diagrams
- Maximum likelihood decoding
- Viterbi algorithm
- Low-Density Parity-Check (LDPC) Codes
- Tanner graph representation
- Message-passing decoding over BEC

## References

1. Joy A Thomas, Thomas M Cover, *Elements of Information Theory*, Wiley-Interscience.
2. David JC McKay, *Information Theory, Inference and Learning Algorithms*, Cambridge University Press.
3. RG Gallager, *Principles of Digital Communication*, Cambridge University Press.
4. Simon Haykin, *Digital Communication Systems*, Wiley.
5. Ron M Roth, *Introduction to Coding Theory*, Cambridge University Press.
6. Shu Lin & Daniel J. Costello Jr., *Error Control Coding: Fundamentals and Applications*, 2nd Edition.
7. Rüdiger Urbanke, TJ Richardson, *Modern Coding Theory*, Cambridge University Press.

