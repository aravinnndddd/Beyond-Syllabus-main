---
country: "india"
university: "ktu"
branch: "computer-science-and-engineering"
version: "2019"
semester: 8
course_code: "cst478"
course_title: "computational-linguistics"
language: "english"
contributor: "@UmarAlMukhtar"
---

# CST478: Computational Linguistics  

## Course Objectives  
* Understand the fundamental concepts of language processing.  
* Apply probability, statistical inference, and hidden Markov models in NLP tasks.  
* Compare and summarize methods for word sense disambiguation, lexical acquisition, and selectional preferences.  
* Implement part-of-speech tagging methods for language modelling.  
* Analyze probabilistic context-free grammars and parsing methods.  
* Develop simple systems for linguistic tasks using Python and NLTK.  

## Modules  

### Module 1 – Preliminaries  
* Rationalist vs. Empiricist Approaches to Language.  
* Linguistics Questions, Noncategorical Phenomena, Language and Cognition as Probabilistic Phenomena.  
* Ambiguity in Language – Lexical Resources, Word Counts, Zipf’s Law, Collocations, Concordances.  
* Linguistic Essentials – Parts of Speech, Morphology, Phrase Structure, Semantics, Pragmatics, Corpus-based Work.  

### Module 2 – Mathematical Essentials  
* Probability Theory – Spaces, Conditional Probability, Bayes' Theorem, Random Variables, Distributions.  
* Statistical Inference – n-gram Models, Equivalence Classes, Reliability vs. Discrimination.  
* Markov Models & Hidden Markov Models – Probability of Observation, Best State Sequence.  

### Module 3 – Word Sense Disambiguation  
* Supervised & Unsupervised Learning, Pseudowords, Upper & Lower Bounds.  
* Supervised Disambiguation – Bayesian Classification.  
* Dictionary-based & Thesaurus-based Disambiguation.  
* Lexical Acquisition, Evaluation Measures, Verb Subcategorization, PP Attachment, Selectional Preferences.  
* Semantic Similarity – Vector Space & Probabilistic Measures.  

### Module 4 – Grammar and POS Tagging  
* Part-of-Speech Tagging – Information Sources, Markov Model Taggers, HMM Taggers, Effect of Initialization.  
* Transformation-Based Learning of Tags.  
* Probabilistic Context-Free Grammars – Features, Probability of a String, Inside & Outside Probabilities, Likely Parse Determination.  

### Module 5 – Language Processing with Python  
* NLTK Introduction – Text Wrangling, Cleansing (Sentence Splitting, Tokenization, Stemming, Lemmatization, Stop-word Removal, Rare Word Removal, Spell Correction).  
* POS Tagging & Named Entity Recognition (NER).  
* Parsing – Shallow vs. Deep Parsing, Types of Parsers, Dependency Parsing.  

## References  
* C.D. Manning, H. Schutze – *Foundations of Statistical Natural Language Processing*, MIT Press.  
* Steven Bird, Ewan Klein, Edward Loper – *Natural Language Processing with Python and NLTK*, O’Reilly.  
* D. Jurafsky, J.H. Martin – *Speech and Language Processing*, PHI.  
* James Allen – *Natural Language Understanding*, Pearson.  
* Nitin Hardeniya, Jacob Perkins, Deepti Chopra, Nisheeth Joshi, Iti Mathur – *Natural Language Processing: Python and NLTK*, Packt Publishing.  
