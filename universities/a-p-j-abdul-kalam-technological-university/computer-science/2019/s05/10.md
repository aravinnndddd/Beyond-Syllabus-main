---
country: "india"
university: "ktu"
branch: "computer-science-and-engineering"
version: "2019"
semester: "5"
course_code: "cst395"
course_title: "neural-networks-and-deep-learning"
language: "english"
contributor: "@UmarAlMukhtar"
---

# CST395: Neural Networks and Deep Learning

## Course Objectives
* Demonstrate the basic concepts of machine learning models and performance measures.  
* Illustrate the basic concepts of neural networks and their practical issues.  
* Outline the standard regularization and optimization techniques for deep neural networks.  
* Build CNN and RNN models for different use cases.  
* Explain the concepts of modern RNNs like LSTM and GRU.  

## Course Content

### **Module 1 – Basics of Machine Learning**
* Overview of machine learning algorithms: Supervised, Unsupervised, Reinforcement learning.  
* Concepts of overfitting, underfitting, hyperparameters, validation sets.  
* Estimators – Bias and Variance, Challenges in ML.  
* Simple Linear Regression, Logistic Regression.  
* Performance measures – Confusion matrix, Accuracy, Precision, Recall, Sensitivity, Specificity, ROC Curve, AUC.  

### **Module 2 – Neural Networks**
* Introduction to neural networks – Single-layer and Multi-Layer Perceptrons (MLPs).  
* Representation power of MLPs.  
* Activation functions – Sigmoid, Tanh, ReLU, Softmax.  
* Risk minimization, Loss function, Training MLPs using backpropagation.  
* Practical issues – Overfitting, Vanishing & Exploding Gradients, Convergence issues, Local minima, Computational challenges.  
* Applications of neural networks.  

### **Module 3 – Deep Learning**
* Deep Feedforward Networks, Training deep models.  
* Optimization techniques – Gradient Descent, Momentum, Nesterov Accelerated GD, Stochastic GD, AdaGrad, RMSProp, Adam.  
* Regularization Techniques – L1/L2 regularization, Early stopping, Data augmentation, Parameter sharing, Noise injection, Ensemble methods, Dropout, Parameter initialization.  

### **Module 4 – Convolutional Neural Networks (CNN)**
* Convolution operations, Motivation, Pooling techniques.  
* Variants of convolution functions, Efficient convolution algorithms.  
* Practical use cases of CNN.  
* **Case Study** – Building CNN model (AlexNet) for MNIST handwritten digit classification.  

### **Module 5 – Recurrent Neural Networks (RNN)**
* RNN design and computational graphs.  
* Encoder–decoder sequence-to-sequence architectures.  
* Deep recurrent networks, Recursive neural networks.  
* Modern RNNs – LSTM, GRU.  
* Practical use cases – **Case Study: Natural Language Processing**.  

## **References**
1. Goodfellow, I., Bengio, Y., and Courville, A., *Deep Learning*, MIT Press, 2016.  
2. Charu C. Aggarwal, *Neural Networks and Deep Learning*, Springer, 2018.  
3. Nikhil Buduma, Nicholas Locascio, *Fundamentals of Deep Learning*, O'Reilly Media, 2017.  
4. Satish Kumar, *Neural Networks: A Classroom Approach*, Tata McGraw-Hill, 2004.  
5. B. Yegnanarayana, *Artificial Neural Networks*, PHI Learning, 2009.  
6. Michael Nielsen, *Neural Networks and Deep Learning*, 2018.  
